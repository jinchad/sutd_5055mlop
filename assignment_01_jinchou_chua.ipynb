{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1cfb22-4b03-4682-9a21-bca5cb05c682",
   "metadata": {},
   "source": [
    "# Assignment 1: detecting offensive content on twitter\n",
    "**Assignment due 1 March 2025 11:59pm**\n",
    "\n",
    "Welcome to the first assignment for 50.055 Machine Learning Operations. These assignments give you a chance to practice the methods and tools you have learned. \n",
    "\n",
    "**This assignment is an individual assignment.**\n",
    "\n",
    "- Read the instructions in this notebook carefully\n",
    "- Add your solution code and answers in the appropriate places. The questions are marked as **QUESTION:**, the places where you need to add your code and text answers are marked as **ADD YOUR SOLUTION HERE**\n",
    "- The completed notebook, including your added code and generated output, will be your submission for the assignment.\n",
    "- The notebook should execute without errors from start to finish when you select \"Restart Kernel and Run All Cells..\". Please test this before submission.\n",
    "- Use the SUTD Education Cluster or Google Colab to solve and test the assignment.\n",
    "\n",
    "**Rubric for assessment** \n",
    "\n",
    "Your submission will be graded using the following criteria. \n",
    "1. Code executes: your code should execute without errors. The SUTD Education cluster should be used to ensure the same execution environment.\n",
    "2. Correctness: the code should produce the correct result or the text answer should state the factual correct answer.\n",
    "3. Style: your code should be written in a way that is clean and efficient. Your text answers should be relevant, concise and easy to understand.\n",
    "4. Partial marks will be awarded for partially correct solutions.\n",
    "5. There is a maximum of 76 points for this assignment.\n",
    "\n",
    "\n",
    "**ChatGPT policy:** \n",
    "\n",
    "If you use AI tools, such as ChatGPT, to solve the assignment questions, you need to be transparent about its use and mark AI-generated content as such. In particular, you should include the following in addition to your final answer:\n",
    "- A copy or screenshot of the prompt you used\n",
    "- The name of the AI model\n",
    "- The AI generated output\n",
    "- An explanation why the answer is correct or what you had to change to arrive at the correct answer\n",
    "\n",
    "**Assignment Notes:** Please make sure to save the notebook as you go along. Submission Instructions are located at the bottom of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "980439b6-d91e-467e-a0b8-45a0d6637c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Installing all required packages\n",
    "# # ----------------\n",
    "# ! pip install transformers[torch]==4.37.2\n",
    "# ! pip install evaluate==0.4.1\n",
    "# ! pip install scikit-learn==1.4.0\n",
    "# ! pip install datasets==2.17.1\n",
    "# ! pip install wandb==0.16.3\n",
    "# ! pip install seaborn==0.13.2\n",
    "# ! pip install peft==0.10.0\n",
    "# ! pip install accelerate==0.28.0 \n",
    "# # ----------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a272f7c4-8dec-4ec8-986c-2ee28ed366f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jin/KC_game/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing all required packages\n",
    "# ----------------\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c6fca",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/bar-plot-in-matplotlib/\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "https://stackoverflow.com/questions/22210768/getting-training-time-in-scikit\n",
    "https://towardsdatascience.com/calculating-a-baseline-accuracy-for-a-classification-model-a4b342ceb88f/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adda5aa5-1297-4767-b36e-05fe6df2cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f84068-a112-46a0-b531-494a903eccae",
   "metadata": {},
   "source": [
    "# Offensive language detection\n",
    "\n",
    "Content moderation of offensive or hateful language is an important task on social media platforms. \n",
    "In this assignment, you will train a text classification models for detecting offensive language on twitter. You will run experiments with different models and evaluate their performance and costs.\n",
    "\n",
    "We will use the TweetEval data set from Barbiert et al (2020): https://aclanthology.org/2020.findings-emnlp.148.pdf\n",
    "\n",
    "\n",
    "**Warning**\n",
    "Some of the content contains rude and offensive language. If you know that this causes you distress, let the course instructor know to arrange a different assessment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "730b9f1f-5862-40c0-8b10-a1b7c23fac90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 11916\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 860\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1324\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data set \n",
    "dataset = load_dataset(\"tweet_eval\", \"offensive\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e89a6973-1615-4d7d-8751-744e140465ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '@user Bono... who cares. Soon people will understand that they gain nothing from following a phony celebrity. Become a Leader of your people instead or help and support your fellow countrymen.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "# QUESTION: print the first training set sample \n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (1 point)---\n",
    "\n",
    "dataset.column_names\n",
    "\n",
    "print(dataset['train'][0])\n",
    "\n",
    "#------------------------------\n",
    "# Hint: you should see a tweet about U2 singer Bono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f8915fe-1ed9-4b4a-82ed-fa34de18427f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER: The possible training values are: [0, 1] \n",
      "\n",
      "This is the sample data for texts with label 1:\n",
      "@user Eight years the republicans denied obama‚Äôs picks. Breitbarters outrage is as phony as their fake president.\n",
      "@user She has become a parody unto herself? She has certainly taken some heat for being such an....well idiot. Could be optic too  Who know with Liberals  They're all optics.  No substance\n",
      "@user Your looking more like a plant #maga #walkaway\n",
      "@user Antifa would burn a Conservatives house down and CNN would be there lighting the torches &amp; throwing gas on the flames.\n",
      "@user They cite Jones being banned for violating Twitter's ToS. There are blue checkmarks spewing the same, if not worse, kind of shit. If you are going to play the anyone can get banned\"\" card. Shouldn't these people also receive bans and suspensions? #VerifiedHate\"\"\n",
      "\n",
      "\n",
      "This is the sample data for texts with label 0:\n",
      "@user Bono... who cares. Soon people will understand that they gain nothing from following a phony celebrity. Become a Leader of your people instead or help and support your fellow countrymen.\n",
      "@user Get him some line help. He is gonna be just fine. As the game went on you could see him progressing more with his reads. He brought what has been missing. The deep ball presence. Now he just needs a little more time\n",
      "@user @user She is great. Hi Fiona!\n",
      "@user @user @user @user @user @user @user @user @user @user @user @user @user @user @user This is the VetsResistSquadron\"\" is Bullshit.. They are girl scout veterans, I have never met any other veterans or served with anyone that was a gun control advocate? Have you?\"\"\n",
      "@user @user Lol. Except he‚Äôs the most successful president in our lifetimes. He‚Äôs undone most of the damage Obummer did and set America on the right path again. #MAGA\n",
      "\n",
      "\n",
      "ANSWER: By analyzing the sample data for data with label 0 and data with label 1, along with the given context that the data consists of rude and offensive language, we can determine text with label value 1 are 'offensive', while text with label value 0 are 'not offensive'. \n"
     ]
    }
   ],
   "source": [
    "# QUESTION: what are the possible values of the labels? What is their meaning? \n",
    "# Print the set of label values and their label names\n",
    "#--- ADD YOUR SOLUTION HERE (5 points) ---\n",
    "label_set = dataset['train'].unique('label')\n",
    "print(f\"ANSWER: The possible training values are: {label_set} \\n\")\n",
    "\n",
    "training_dataset = dataset['train']\n",
    "training_df = training_dataset.to_pandas()\n",
    "label_1_mask = training_df['label'].isin([1])\n",
    "label_0_mask = training_df['label'].isin([0])\n",
    "\n",
    "label_1_sample = training_df[label_1_mask]['text'].head().to_list()\n",
    "label_0_sample = training_df[label_0_mask]['text'].head().to_list()\n",
    "\n",
    "print(\"This is the sample data for texts with label 1:\")\n",
    "for text in label_1_sample:\n",
    "    print(text)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"This is the sample data for texts with label 0:\")\n",
    "for text in label_0_sample:\n",
    "    print(text)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"ANSWER: By analyzing the sample data for data with label 0 and data with label 1, along with the given context that the data consists of rude and offensive language, we can determine text with label value 1 are 'offensive', while text with label value 0 are 'not offensive'. \")\n",
    "# -------\n",
    "# Hint: it is a binary task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdbf3671-1066-4458-92fd-dc1932094187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Label count for training dataset')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# QUESTION: plot a bar chart of the label distribution\n",
    "#--- ADD YOUR SOLUTION HERE (5 points) ---\n",
    "sns.countplot(x=dataset['train']['label'])\n",
    "plt.xlabel('Label count for training dataset')\n",
    "\n",
    "#------------------------------\n",
    "# Hint: it is not evenly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f73f29c-9728-4a19-8d5a-9e7440d56f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: separate data set into training, validation and test according to given dataset split\n",
    "# You should end up with the following variables\n",
    "# train_text = array containing strings in training set\n",
    "# train_labels = array containing numeric labels in training set\n",
    "# validation_text = array containing strings in training set\n",
    "# validation_labels = array containing numeric labels in training set\n",
    "# test_text = array containing strings in training set\n",
    "# test_labels = array containing numeric labels in training set\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (10 points) ---\n",
    "\n",
    "train_text = dataset['train']['text']\n",
    "train_labels = dataset['train']['label']\n",
    "\n",
    "validation_text = dataset['validation']['text']\n",
    "validation_labels = dataset['validation']['label']\n",
    "\n",
    "test_text = dataset['test']['text']\n",
    "test_labels = dataset['test']['label']\n",
    "\n",
    "#------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4254bcf4-3926-4989-99a4-6985e3410caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train:  11916\n",
      "#validation:  1324\n",
      "#test:  860\n"
     ]
    }
   ],
   "source": [
    "# check the size of the data splits\n",
    "print(\"#train: \", len(train_text)) \n",
    "print(\"#validation: \", len(validation_text)) \n",
    "print(\"#test: \", len(test_text)) \n",
    "\n",
    "# Hint: you should see\n",
    "#train:  11916\n",
    "#validation:  1324\n",
    "#test:  860"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9562c182-b658-49b3-bcd3-852052dc7e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# QUESTION: create a scikit-learn pipeline object that creates unigram features, applies tf-idf weighting and trains a SGDClassifier \n",
    "# tf-idf stands for ‚ÄúTerm Frequency times Inverse Document Frequency‚Äù.\n",
    "# tf-idf is a feature weighting methods commonly used in NLP and IR\n",
    "# use default parameters for unigram feature extraction, tf-idf and the SGDClassifier\n",
    "# add additional import statements in this cell as needed\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (10 points) ---\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('unigram', CountVectorizer()),\n",
    "    ('tf-idf', TfidfTransformer()),  \n",
    "    ('sgd', SGDClassifier())  \n",
    "])\n",
    "\n",
    "#------------------------------\n",
    "# Hint: use the scikit-learn library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f27bb73-f483-4364-9b9a-59493031cfcf",
   "metadata": {},
   "source": [
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54deb077-2ede-425c-91f8-32d24381f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# QUESTION: apply your pipeline of feature extraction and model training to the training set\n",
    "# Measure the wall-clock training time needed \n",
    "# Store the training time in a variable 'train_time_sgd\n",
    "#--- ADD YOUR SOLUTION HERE (5 points) ---\n",
    "\n",
    "start_time = time.time()\n",
    "pipeline.fit(X = train_text, y = train_labels)\n",
    "pipeline.memory\n",
    "end_time = time.time()\n",
    "\n",
    "train_time_sgd = end_time-start_time\n",
    "#------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3de2987f-83f1-4948-a390-05ed2517f151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.19431018829345703s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training time: {train_time_sgd}s\")\n",
    "\n",
    "# Hint: training should take < 1 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0637838e-9741-4c6f-a889-e76dfbe25ead",
   "metadata": {},
   "source": [
    "# Test the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2c2a4a7-78ea-4942-afe3-cf20d177eca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The majority baseline score of validation set and test set are 0.6533232628398792 and 0.7209302325581395 respectively. \n",
      "\n",
      "The precision for the test dataset for the class with label 0 is: 0.7209302325581395\n",
      "The recall for the test dataset for the class with label 0 is: 1.0\n",
      "The f1 score for the test dataset for the class with label 0 is: 0.8378378378378378\n",
      "\n",
      "The precision for the test dataset for the class with label 1 is: 0.0\n",
      "The recall for the test dataset for the class with label 1 is: 0.0\n",
      "The f1 score for the test dataset for the class with label 1 is: 0.0\n",
      "\n",
      "The precision for the validation dataset for the class with label 0 is: 0.6533232628398792\n",
      "The recall for the validation dataset for the class with label 0 is: 1.0\n",
      "The f1 score for the validation dataset for the class with label 0 is: 0.7903152124257652\n",
      "\n",
      "The precision for the validation dataset for the class with label 1 is: 0.0\n",
      "The recall for the validation dataset for the class with label 1 is: 0.0\n",
      "The f1 score for the validation dataset for the class with label 1 is: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# QUESTION: compute the majority class baseline score on the validation set and test set\n",
    "# the majority class baseline is the score you get if you always predict the most frequent label\n",
    "# \n",
    "# Compute the precision, recall and F1 score for the majority baseline for validation and test set for each class\n",
    "#\n",
    "#--- ADD YOUR SOLUTION HERE (5 points) ---\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_majority_baseline(data_labels: list) -> tuple[float, list]:\n",
    "    label1 = data_labels.count(1)\n",
    "    label0 = data_labels.count(0)\n",
    "    total_labels = len(data_labels)\n",
    "\n",
    "    if label1 > label0:\n",
    "        return label1/total_labels, [1]*total_labels\n",
    "    else:\n",
    "        return label0/total_labels, [0]*total_labels\n",
    "\n",
    "\n",
    "val_majority_baseline_score, val_majority_baseline_prediction = calculate_majority_baseline(validation_labels)\n",
    "test_majority_baseline_score, test_majority_baseline_prediction = calculate_majority_baseline(test_labels)\n",
    "\n",
    "print(f\"The majority baseline score of validation set and test set are {val_majority_baseline_score} and {test_majority_baseline_score} respectively. \\n\")\n",
    "\n",
    "val_precision = precision_score(y_true=validation_labels, y_pred=val_majority_baseline_prediction, average= None, labels=[0,1], zero_division=0)\n",
    "val_recall = recall_score(y_true=validation_labels, y_pred=val_majority_baseline_prediction, average= None, labels=[0,1], zero_division=0)\n",
    "val_f1 = f1_score(y_true=validation_labels, y_pred=val_majority_baseline_prediction, average= None, labels=[0,1], zero_division=0)\n",
    "\n",
    "test_precision = precision_score(y_true=test_labels, y_pred=test_majority_baseline_prediction, average= None, labels=[0,1], zero_division=0)\n",
    "test_recall = recall_score(y_true=test_labels, y_pred=test_majority_baseline_prediction, average= None, labels=[0,1], zero_division=0)\n",
    "test_f1 = f1_score(y_true=test_labels, y_pred=test_majority_baseline_prediction, average= None, labels=[0,1], zero_division=0)\n",
    "\n",
    "for idx in range(len(test_precision)):\n",
    "    print(f\"The precision for the test dataset for the class with label {idx} is: {test_precision[idx]}\")\n",
    "    print(f\"The recall for the test dataset for the class with label {idx} is: {test_recall[idx]}\")\n",
    "    print(f\"The f1 score for the test dataset for the class with label {idx} is: {test_f1[idx]}\\n\")\n",
    "\n",
    "for idx in range(len(val_precision)):\n",
    "    print(f\"The precision for the validation dataset for the class with label {idx} is: {val_precision[idx]}\")\n",
    "    print(f\"The recall for the validation dataset for the class with label {idx} is: {val_recall[idx]}\")\n",
    "    print(f\"The f1 score for the validation dataset for the class with label {idx} is: {val_f1[idx]}\\n\")\n",
    "\n",
    "#------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1a2cb65-1d21-4df8-b339-adc157eef182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the predictions for the validation dataset is: 0.7643504531722054\n",
      "The precision of the predictions for the validation dataset is: 0.8154506437768241\n",
      "The recall of the predictions for the validation dataset is: 0.4139433551198257\n",
      "The f1 score of the predictions for the validation dataset for the 'positive' class is: 0.5491329479768786\n",
      "\n",
      "The accuracy of the predictions for the test dataset is: 0.8011627906976744\n",
      "The precision of the predictions for the test dataset is: 0.8\n",
      "The recall of the predictions for the test dataset is: 0.38333333333333336\n",
      "The f1 score of the predictions for the test dataset for the 'positive' class is: 0.5183098591549296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# \n",
    "# QUESTION: now use your pipeline to make predictions on validation and test set\n",
    "# compute and print accuracy, precision, recall, F1 score\n",
    "# \n",
    "# From now on, we are only concerned with the F1 score for the \"positive\" class which are the offensive tweets\n",
    "# Store the test F1 score for the \"positive\" class in a variable 'f1_validation_sgd' and 'f1_test_sgd' for validation and test set, respectively \n",
    "#--- ADD YOUR SOLUTION HERE (10 points) ---\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_pred = pipeline.predict(X = validation_text)\n",
    "test_pred = pipeline.predict(X = test_text)\n",
    "\n",
    "val_accuracy = accuracy_score(y_true=validation_labels, y_pred=val_pred)\n",
    "val_precision = precision_score(y_true=validation_labels, y_pred=val_pred, zero_division=0)\n",
    "val_recall = recall_score(y_true=validation_labels, y_pred=val_pred, zero_division=0)\n",
    "f1_validation_sgd = f1_score(y_true=validation_labels, y_pred=val_pred, pos_label=1, zero_division=0)\n",
    "\n",
    "print(f\"The accuracy of the predictions for the validation dataset is: {val_accuracy}\")\n",
    "print(f\"The precision of the predictions for the validation dataset is: {val_precision}\")\n",
    "print(f\"The recall of the predictions for the validation dataset is: {val_recall}\")\n",
    "print(f\"The f1 score of the predictions for the validation dataset for the 'positive' class is: {f1_validation_sgd}\\n\")\n",
    "\n",
    "test_accuracy = accuracy_score(y_true=test_labels, y_pred=test_pred)\n",
    "test_precision = precision_score(y_true=test_labels, y_pred=test_pred, zero_division=0)\n",
    "test_recall = recall_score(y_true=test_labels, y_pred=test_pred, zero_division=0)\n",
    "f1_test_sgd = f1_score(y_true=test_labels, y_pred=test_pred, pos_label=1, zero_division=0)\n",
    "\n",
    "print(f\"The accuracy of the predictions for the test dataset is: {test_accuracy}\")\n",
    "print(f\"The precision of the predictions for the test dataset is: {test_precision}\")\n",
    "print(f\"The recall of the predictions for the test dataset is: {test_recall}\")\n",
    "print(f\"The f1 score of the predictions for the test dataset for the 'positive' class is: {f1_test_sgd}\\n\")\n",
    "\n",
    "#------------------------------\n",
    "# Hint: F1 scores should be >50%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7958c9e-6aab-4066-a0f0-0296e08c0935",
   "metadata": {},
   "source": [
    "# BERT model\n",
    "\n",
    "Now let us try a more powerful model: the DistilBERT uncased model\n",
    "\n",
    "https://huggingface.co/distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106f683-1c93-414f-b7ba-31a3722367da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1324/1324 [00:00<00:00, 9838.72 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# load DistilBERT tokenizer and tokenize data set\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09087fb6-1cc6-457c-b601-316d7bb3b3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load DistilBERT model for classification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (5 points) ---\n",
    "num_labels = len(np.unique(dataset['train']['label']))\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "#------------------------------\n",
    "# Hint: make sure your model corresponds to your tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d865dca-a413-4060-ac4e-a49039262f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add custom metrics that computes precision, recall, f1, accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "   # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(labels, preds, average='binary')\n",
    "    recall = recall_score(labels, preds, average='binary')\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcbf727-1479-4b81-988a-c18eba4de126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jin/KC_game/venv/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#\n",
    "# QUESTION: configure the training parameters using the Huggingface TrainingArguments class\n",
    "# - set the output directory to \"finetuning-tweeteval\"\n",
    "# - do not report training metrics to an external experiment tracking service\n",
    "# - print acc/p/r/f1 scores on the validation set every 200 steps\n",
    "# - learning rate to 2e-5, \n",
    "# - set weight decay to 0.01\n",
    "# - set epochs to 1\n",
    "\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (5 points) ---\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"finetuning-tweeteval\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=200)\n",
    "\n",
    "#------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef1bb5-565d-4d9d-ad4a-cfba3f6d47d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d891ae-4a62-44d1-8cf1-d05a1bc75074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchuajinchou\u001b[0m (\u001b[33mchuajinchou-singapore-university-of-technology-and-desig\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jin/Downloads/wandb/run-20250301_143542-sr4xez1g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/chuajinchou-singapore-university-of-technology-and-desig/huggingface/runs/sr4xez1g' target=\"_blank\">finetuning-tweeteval</a></strong> to <a href='https://wandb.ai/chuajinchou-singapore-university-of-technology-and-desig/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/chuajinchou-singapore-university-of-technology-and-desig/huggingface' target=\"_blank\">https://wandb.ai/chuajinchou-singapore-university-of-technology-and-desig/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/chuajinchou-singapore-university-of-technology-and-desig/huggingface/runs/sr4xez1g' target=\"_blank\">https://wandb.ai/chuajinchou-singapore-university-of-technology-and-desig/huggingface/runs/sr4xez1g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1490' max='1490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1490/1490 15:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.546200</td>\n",
       "      <td>0.559592</td>\n",
       "      <td>0.719033</td>\n",
       "      <td>0.564829</td>\n",
       "      <td>0.825708</td>\n",
       "      <td>0.670796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.486100</td>\n",
       "      <td>0.455266</td>\n",
       "      <td>0.794562</td>\n",
       "      <td>0.771014</td>\n",
       "      <td>0.579521</td>\n",
       "      <td>0.661692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.466600</td>\n",
       "      <td>0.447655</td>\n",
       "      <td>0.791541</td>\n",
       "      <td>0.710345</td>\n",
       "      <td>0.673203</td>\n",
       "      <td>0.691275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.431000</td>\n",
       "      <td>0.456578</td>\n",
       "      <td>0.799094</td>\n",
       "      <td>0.774929</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.671605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.448200</td>\n",
       "      <td>0.445006</td>\n",
       "      <td>0.789275</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.651685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>0.432451</td>\n",
       "      <td>0.796073</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.614379</td>\n",
       "      <td>0.676259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.428866</td>\n",
       "      <td>0.803625</td>\n",
       "      <td>0.726651</td>\n",
       "      <td>0.694989</td>\n",
       "      <td>0.710468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model\n",
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003379b3-a5ba-4d80-8998-514d57bf2449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1764' max='1490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1490/1490 04:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3610236942768097,\n",
       " 'eval_accuracy': 0.8480194696206781,\n",
       " 'eval_precision': 0.77734375,\n",
       " 'eval_recall': 0.7574219741182441,\n",
       " 'eval_f1': 0.7672535663796427,\n",
       " 'eval_runtime': 236.2027,\n",
       " 'eval_samples_per_second': 50.448,\n",
       " 'eval_steps_per_second': 6.308,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on training set\n",
    "trainer.evaluate(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeb0514-431e-44b0-8372-c57e354bc330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.42928385734558105,\n",
       " 'eval_accuracy': 0.8021148036253777,\n",
       " 'eval_precision': 0.7193763919821826,\n",
       " 'eval_recall': 0.7037037037037037,\n",
       " 'eval_f1': 0.711453744493392,\n",
       " 'eval_runtime': 26.5726,\n",
       " 'eval_samples_per_second': 49.826,\n",
       " 'eval_steps_per_second': 6.247,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "trainer.evaluate(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51d1460-504c-4ab3-8eaf-0691f1f5a91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss: 0.3737415075302124\n",
      "eval_accuracy: 0.8441860465116279\n",
      "eval_precision: 0.7431192660550459\n",
      "eval_recall: 0.675\n",
      "eval_f1: 0.7074235807860262\n",
      "eval_runtime: 17.186\n",
      "eval_samples_per_second: 50.041\n",
      "eval_steps_per_second: 6.284\n",
      "epoch: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_output = trainer.evaluate(test_dataset)\n",
    "for key in test_output:\n",
    "    print(f\"{key}: {test_output[key]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d39da-72df-410e-adb4-325c6be590d4",
   "metadata": {},
   "source": [
    "### QUESTION: \n",
    "Do you see any signs of overfitting or underfitting based on the evaluation scores\n",
    "Explain why or why not\n",
    "\n",
    "**--- ADD YOUR SOLUTION HERE (5 points) ---**\n",
    "\n",
    "There are no signs of overfitting or underfitting. Comparing the accuracy from all 3 evaluations, we find that the model's performance was somewhat close with just a ~4% difference in accuracy from the validation set and the training set. Next, comparing the model's loss on all 3 evaluations, we also find that the model's performance was roughly around 0.3 to 0.4, which is another sign that there is unlikely any sign of overfitting or underfitting of the model. Finally, the precision and recall of all 3 evaluation sets are very close. \n",
    "\n",
    "With these findings, we can conclude that there are no signs of overfitting or underfitting based on the evaluation scores.\n",
    "\n",
    "------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82913fe-0bbc-41de-acae-ad0662926b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio for SGD model: 2.7669286009031966\n",
      "Ratio for bert model: 0.0007723897457335199\n",
      "0.5307262569832403 0.7074235807860262\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# QUESTION: What is the ratio f1 score to training time for the SGDClassifier and the DistilBERT model\n",
    "# compute the two ratios and print them\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE ---\n",
    "train_time_bert = train_output.metrics[\"train_runtime\"]\n",
    "f1_test_bert = test_output['eval_f1']\n",
    "\n",
    "ratio_sgd = f1_test_sgd/train_time_sgd\n",
    "ratio_bert = f1_test_bert/train_time_bert\n",
    "\n",
    "print(f\"Ratio for SGD model: {ratio_sgd}\")\n",
    "print(f\"Ratio for bert model: {ratio_bert}\")\n",
    "#------------------------------\n",
    "print(f1_test_sgd, f1_test_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e7d14-f997-4022-ab46-2cb1ac247073",
   "metadata": {},
   "source": [
    "### QUESTION: \n",
    "Given the results what model would you recommend to use? Write a paragraph (max 200 words) to explain your choice\n",
    "\n",
    "**--- ADD YOUR SOLUTION HERE (10 points)---**\n",
    "\n",
    "I would recommend to the BERT model. Given the context that this model is used to detect offensive language in posts, more emphasis should be placed on the true positive and false negatives of the model's performance. The reprecussions from falsely identifying an offensive post as \"non offensive\" is far more severe than falsely identifying a safe post as offensive as it could potentially lead to more dangerous behaviour. With that, the BERT model had a much higher F1 score than the sgd model, hence indicating that it is more capable of detecting offensive posts and hence I recommend it.\n",
    "\n",
    "It should be noted that this answer assumes that there are adequete resources to train the BERT model. If training resources are very limited, it could be more beneficial to experiment with the SGD model.\n",
    "\n",
    "------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc57acf-c65a-4ba7-9d9c-9b212251e542",
   "metadata": {},
   "source": [
    "# End\n",
    "\n",
    "This concludes assignment 1.\n",
    "\n",
    "\n",
    "Please submit this notebook with your answers and the generated output cells as a **Jupyter notebook file** via github.\n",
    "\n",
    "\n",
    "1. Create a private github repository **sutd_5055mlop** under your github user.\n",
    "2. Add your instructors as collaborator: ddahlmeier and lucainiaoge\n",
    "3. Save your submission as assignment_01_STUDENT_NAME.ipynb where STUDENT_NAME is your name in your SUTD email address.\n",
    "4. Push the submission file to your repo \n",
    "5. Submit the link to the repo via eDimensions\n",
    "\n",
    "Example:<br/>\n",
    "Email: michael_tan@mymail.sutd.edu.sg<br/>\n",
    "STUDENT_NAME: michael_tan<br/>\n",
    "Submission file name: assignment_01_michael_tan.ipynb\n",
    "\n",
    "\n",
    "\n",
    "**Assignment due 01 March 2025 11:59pm**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc1be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
